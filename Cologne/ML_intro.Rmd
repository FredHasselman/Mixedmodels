---
title: "Generalized Mixed Models"
author: "Fred Hasselman"
date: "02/06/2017"
output:
  html_document:
    fig_caption: yes
    fig_height: 8
    fig_width: 11
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    fig_caption: yes
    fig_crop: no
    fig_height: 8
    fig_width: 11
    highlight: zenburn
    keep_tex: yes
    latex_engine: xelatex
    toc: yes
    toc_depth: 4
monofont: Gill Sans Light
mainfont: Calibri
sansfont: Gill Sans Light
geometry: a4paper
---

```{r set-options, echo=FALSE,include=FALSE}
require(knitr)
require(formatR)
require(devtools)
options(width=300)
knitr::opts_chunk$set(cache=FALSE,prompt=FALSE,comment=NA,message=FALSE,echo=TRUE,warning=FALSE,tidy=FALSE,strip.white=FALSE,size="small")
```



# **Introduction to the GMM family**

Many different terms are used to describe some form of the hierarchical *Generalized Mixed Model* family of statistical models, or *GeMMs*  for short (pronounce like the precious stones, *gems*). 

## What's in a name?

* _Generalized_ refers to the fact that we can use any distribution from [the exponential family](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions) as a model for our response variable(s), because we know the [functions that link](https://en.wikipedia.org/wiki/Hierarchical_generalized_linear_model#Models_with_different_distributions_and_link_functions) the parameters of those distributions. 
* _Mixed_ refers to possibility to have different types of effects in the model, *fixed* and *random* effects.
    + Random effect structures can be used to model dependencies in the data ([heteroscedasticity](http://www.dummies.com/education/economics/econometrics/how-to-distinguish-between-homoskedastic-and-heteroskedastic-disturbances/)) that affects the eficiency and accuracy of OLS estimators.
    + Random effect structures can be *simple*, *autoregressive*, hierarchically *nested*, *cross-classified*, of the *multiple membership* kind or any combination of those. 
* _GeMMs_ can be *linear* or *nonlinear* models, and to estimate model parameters (OLS cannot be used) different methods are available; *Maximum Likelihood* estimators, *MCMC* and *Bayesian* methods. There are also extensions based on *vector autoregression (VAR)* and *LASSO*.


### When to use GeMMs? {-}

**Always**  

Well, ok, have a look at the different [multilevel structures one can encounter in real data](http://www.bristol.ac.uk/cmm/learning/multilevel-models/data-structures.html)


### Why? {-}

The *General Linear Model*  (the classical regression models and ANOVA variations) are just special cases of GeMMs in which the following assumptions hold (cf. Field):

* _Linear additivity_ of effects
* _Normality_ something or other
* _Homogeneity_ of variance
* _Independence_ of observations

For real world data, especially in the social and life sciences, at least one of those assumptions will be violated. So the answer to the *Why?* question is: **Because we need more realistic statistical models!**.

## `R` packages

We'll use the `lme4` package and some additional tools.

On thing you will notice is that `lme4` no longer provides p-values for fixed effects, here's what the authors have to say about it:

>  One of the most frequently asked questions about 'lme4' is "how do I calculate p-values for estimated parameters?" Previous versions of `lme4` provided the `mcmcsamp` function, which efficiently generated a Markov chain Monte Carlo sample from the posterior distribution of the parameters, assuming flat (scaled likelihood) priors. Due to difficulty in constructing a version of 'mcmcsamp' that was reliable even in cases where the estimated random effect variances were near zero (e.g. <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q4/003115.html> `mcmcsamp` has been withdrawn (or more precisely, not updated to work with `lme4` versions greater than 1.0.0).



```{r, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
install.packages(c("lme4","lmerTest","pbkrtest","nlme","sjPlot"), dependencies = TRUE, repos = "https://cloud.r-project.org")
```


We'll also use some standard packages for plotting data and model results.
```{r, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
install.packages(c("lattice","latticeExtra","ggplot2","gridExtra","scales"), dependencies = TRUE, repos = "https://cloud.r-project.org")
```


These libraries are not essential, but generally will enhance you `R` experience.
```{r, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
install.packages(c("plyr","tidyverse","rio"), dependencies = TRUE, repos = "https://cloud.r-project.org")
```


## Online sources

Here is a (non-exhaustive) list of great sources:

* [lme4: Mixed-effects modeling with R (Bates)](http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf)
    + [lme4 vignette](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf)
    + [Mixed models in R using `lme4` and `nlme`](https://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Mixed-Models.pdf)
    + [Writing up *lmer* results](https://web.stanford.edu/class/psych253/section/section_8/lmer_examples.html)
    + [Getting started with mixed effect models in r (Knowles, R-bloggers)](https://www.r-bloggers.com/getting-started-with-mixed-effect-models-in-r/)
    + [General guide to mixed models in r](http://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html)
* [Center for multilevel modeling](http://www.bristol.ac.uk/cmm/)
    + From the creators of MLwiN
* [Multilevel Analysis (Hox)](https://stats.idre.ucla.edu/other/examples/ma-hox/)
    + Code examples from book in `HLM`, `SAS`, `Stata`, `R`
* [Introduction to Multilevel Modeling (Kreft & de Leeuw)](https://stats.idre.ucla.edu/other/examples/imm/)
    + Code examples from book in `HLM`, `R`, `SPSS`, `Mplus`, `SAS`, `Stata`
* [Applied Longitudinal Data Analysis (Singer & Willett)](https://stats.idre.ucla.edu/other/examples/alda/)
    + Code examples from book in `R`, `SPSS`, `Mplus`, `SAS`, `Stata`
* [Using lme/lmer to fit 2 and 3 level longitudinal models (Rpsychologist)](http://rpsychologist.com/r-guide-longitudinal-lme-lmer)
    + Excellent site with many examples
* [Inferential Methods for Linear Mixed Models](http://www.maths.bath.ac.uk/~jjf23/mixchange/index.html)
    + Includes examples for multivariate mutilevel modeling
* [R to MLwiN](https://www.jstatsoft.org/article/view/v072i10)
* [Power Analysis for Random Effects Models (Westfall)](http://jakewestfall.org/publications/JWK_AnnRev.pdf)
    + Provides `R`, `SPSS` and `SAS` code
    + Also see the [online tool](https://jakewestfall.shinyapps.io/two_factor_power/) and [additional topics supplement](http://jakewestfall.org/publications/JWK_AnnRev_Appendix.pdf)
* [Stan. Bayesian inference for multilevel GLMMs and more](http://mc-stan.org/interfaces/)
* [Generalized Additive Mixed Models (GAMM) for for modeling timeseries data](http://www.sfs.uni-tuebingen.de/~jvanrij/Tutorial/GAMM.html)
    + [Examples](http://www.sfs.uni-tuebingen.de/~hbaayen/publications/supplementCave.pdf)
* [Multilevel Analysis (Snijder & Boskers)](https://stats.idre.ucla.edu/other/examples/ma-snijders/)
    + Code examples from book in `HLM`, `Mplus`, `SAS`, `Stata`


# Classical linear regression  {.tabset .tabset-fade .tabset-pills}  

OLS regression of the example on the slides.

## Assignment
 
Let's repeat the example form the slides. Import the data from [Github](https://github.com/FredHasselman/Mixedmodels/tree/master/Cologne/AssignmentData), or run the code below and load it directly into R^[Importing SPSS files into R can sometimes lead to strange behaviour, because SPSS assigns both *variable* and *value* labels, which most software packages cannot handle. If you can't get rid of the labels, use `rio::export(df,"filename.ext")` and set `.ext` to `.csv` or `.xlsx`, then reload the file using `df <- rio::import("filename.ext")`].

```{r}
library(rio)
df <- rio::import("https://github.com/FredHasselman/Mixedmodels/raw/master/Cologne/AssignmentData/IntroductionData.sav")

# Change some variables to nominal factors
df$student.f <- factor(df$student)
df$school.f  <- factor(df$school, levels = 1:4, labels = paste("school",1:4))
df$style.f   <- factor(df$style, levels = c(-.5,.5), labels = c("informal style","formal style"))

# To mimic the slides, we analyse relative to school 4
df$school.f <- relevel(df$school.f, ref = "school 4")
```

### Look at the data. 

+ Plot language ability `x` (at the start of the year) against `y` (at the end of the year) and mark each school with a shape or colour. 
+ If you are new to `R`, just look at the solution and copy and run the code. Plot the predicted values.

### Fit the global model.
Using the basic `R` stats functions we can fit a regression model for the entire sample: $Y_{i} = \beta_{0} + \beta_{1} X_{i} + \varepsilon_{i},\ \text{with}\ i = 1,2,\ldots,20$.

+ Use the function `lm`. 
+ If you are unfamiliar with the function, look at the manual entry by typing `?lm` in the console.
+ Plot the predicted values.

### Fit the model with intercepts for schools.

This means adding the `school.f` variable. The use of factors in `R` is very convenient, you do not have to create dummy variables, `R` will do this for you. So we are looking for a model of the type: $Y_{i} = \beta_{0j} + \beta_{1} X_{ij} + \varepsilon_{ij},\ \text{with}\ i = 1,2,\ldots,20\ \text{and}\ j = 1,2,\ldots,4$. 

+ Plot the predicted values.

### Fit the model with intercepts for teaching style.

To fit the model with an intercept for each teaching style, we simply add the factor `style.f` variable. So we are looking for a model of the type: $Y_{ij} = \beta_{0k} + \beta_{1} X_{ik} + \varepsilon_{ik},\ \text{with}\ i = 1,2,\ldots,20\ \text{and}\ k = 1,2$. 

+ On the slides, the model was fitted to the grand-mean centered version of `x`. We will get back to centering data, the variable is in the dataset as `xc`. You could also calculate the variable yoursef using the function `scale`:
```{r, eval=FALSE}
df$xc <- scale(df$x, scale = FALSE)
```

+ Plot the predicted values.

### Fit the model with both school and teaching style effects.

To fit the model with both `school` and `teaching style` effects and just 1 random error term, we need dummy variables. They are already in the dataset. We should get a model of the type: $Y_{i} = \beta_{0} + \beta_{1} X_{i} + \beta_{2}\ Style_{i} + \beta_{3}\ d1_{i} + \beta_{4}\ d2_{i} +  \varepsilon_{i},\ \text{with}\ i = 1,2,\ldots,20$.

+ Plot the predicted values.

### Compare the models using the function `anova()`.



## Solution

### Plot the data
```{r q1, collapse=TRUE}
# Using ggplot2
library(ggplot2)
ggplot(df, aes(x=x,y=y,group=school.f)) +
  geom_point(aes(colour=style.f,shape=school.f), size=5) +
  theme_bw() 
```

### Fit the global model.
```{r q2}
# Fit the model and display a summary of the results
fit1 <- lm(y ~ x, data=df)
summary(fit1)

# Plot fit 1
df$pred1 <- predict(fit1)
ggplot(df, aes(x=x,y=pred1, colour = school.f, shape=school.f)) +
  geom_point(aes(y=y)) +
  geom_line(aes(group=school.f)) +
  ggtitle("Model 1", subtitle = fit1$call) +
  theme_bw()
```

### Fit the model with intercepts for schools.
```{r q3}
# Fit the model and display a summary of the results
fit2 <- lm(y ~ x + school.f, data=df)
summary(fit2)

# Plot fit 2
df$pred2 <- predict(fit2)
ggplot(df, aes(x=x,y=pred2, colour = school.f, shape=school.f)) +
  geom_point(aes(y=y)) +
  geom_line(aes(group=school.f)) +
  ggtitle("Model 2", subtitle = fit2$call) +
  theme_bw()
```

### Fit the model with intercepts for teaching style.
```{r q4}
# Fit the model and display a summary of the results
fit3 <- lm(y ~ xc + style.f, data=df)
summary(fit3)

# Plot fit 3
df$pred3 <- predict(fit3)
ggplot(df, aes(x=xc,y=pred3,colour=style.f,shape=school.f)) +
  geom_point(aes(y=y)) +
  geom_line(aes()) +
  ggtitle("Model 3", subtitle = fit3$call) +
  theme_bw()
```

### Fit the model with an both school and teaching style effects.
```{r q5}
# Fit the model and display a summary of the results
fit4 <- lm(y ~ xc + style.f + d1 + d2, data=df)
summary(fit4)

# Plot fit 4
df$pred4 <- predict(fit4)
ggplot(df, aes(x=xc,y=pred4,shape=style.f,colour=school.f)) +
  geom_point(aes(y=y)) +
  geom_line() +
  ggtitle("Model 4", subtitle = fit4$call) +
  theme_bw()
```

### Compare model fit
```{r q6}
# Compare the models
anova(fit1,fit2, fit3, fit4)
```


# Variance Components  {.tabset .tabset-fade .tabset-pills}  

Now we will start using random effects...

## Assignment 

### The empty multilevel model

Fit a model that considers the variation in the intercepts of the schools as a random variation around the grand mean.

+ Schools are Level 2: We assume the school intercepts can be considered to be drawn from a normal distribution with $\mu = 0$ and unknown $\sigma_{u}^2$. Parameter $\sigma_{u}^2$ will be estimated from the data. 
+ Student scores will be considered to vary randomly around the mean of their respective schools. We will assume students in each school vary around the school mean in the same way, a normal distribution with $\mu = 0$ and unknown $\sigma_{e}^2$.
+ Use the function `lmer()` to model the data. It looks similar to `lm()` but you need to specify the random effect structure. Look at the manual, or if you can't get it to work, at the solutions.

### Adding the random effect structure

+ Add student as a random level: `(1|student.f)` and fit the model
    - The `1` refers to a constant, an intercept
    - The variable after `|` is the variable that groups togethter the data on this level.
+ Add school as a random level: `(1|student.f) + (1}school.f)` and fit the model
+ Indicate that students are in fact nested within schools: `(1|school.f/student.f)`
     - There are more ways this can be achieves see 


### Add teaching style as a factor.

+ Compare the the single level model to the variance components model


## Solution

### The empty multilevel model

Variance components, seperate random effects for students and schools.
```{r q7a, message=FALSE, warning=FALSE}
library(lme4)
library(lmerTest)

# Only students
fit50 <- lmer(y ~ (1|student.f) ,data = df)
summary(fit50)

fit5a <- lmer(y ~ (1|school.f) + (1|student.f) ,data = df)
summary(fit5a)
````

Variance components with students nested within schools.
```{r q7b, message=FALSE, warning=FALSE}

fit5b <- lmer(y ~ (1|school.f/student.f) ,data = df)
summary(fit5a)
````


### Add pre-measure and teachig style
```{r q7c, message=FALSE, warning=FALSE}
# Add teaching style
fit5c <- lmer(y ~ xc + style.f + (1|school.f/student.f) ,data = df)
summary(fit5c)
```

### Compare the models
```{r q7d, message=FALSE, warning=FALSE}
anova(fit50,fit5a,fit5b,fit5c)
```


### Diagnostic plots

```{r q7e, message=FALSE, warning=FALSE}
# Using plot
plot(fit5c, resid(., scaled=TRUE) ~ fitted(.) | style.f, abline = 0)

# box-plots of residuals by school
plot(fit5c, school.f ~ resid(., scaled=TRUE))
# observed versus fitted values by school
plot(fit5c, y ~ fitted(.) | school.f, abline = c(0,1))
# residuals by x, separated by school
plot(fit5c, resid(., scaled=TRUE) ~ x | school.f, abline = 0)

# Use package lattice
require("lattice")
qqmath(fit5c, id=0.05)
```

### Profiling

```{r q7f, message=FALSE, warning=FALSE}
pr <- profile(fit5c, optimizer="Nelder_Mead", which="beta_")

# Profiled confidence intervals
(confint(pr) -> CIpr)
# Zeta functions
xyplot(pr, absVal=TRUE)
xyplot(pr, conf = c(0.95, 0.99), main = "95% and 99% profile() intervals")
# Density plots
densityplot(pr, main="densityplot( profile(lmer(..)) )")
# correlation matrix
splom(pr)

```

### Plot random and fixed effects 

Use the `strenge jacke` package ([sjPlot](http://www.strengejacke.de/sjPlot/sjp.lmer/)).
```{r q8, message=FALSE, warning=FALSE}
library(sjPlot)
# Random effects
sjp.lmer(fit5c, sort.est = "sort.all", facet.grid = FALSE)

# Plot fixed effects
sjp.lmer(fit5c, type = "fe")

# Plot fixed effects
sjp.lmer(fit5c, type = "fe")

# Plot standardised fixed effects
sjp.lmer(fit5c, type = "fe.std")

# plot fixed effects slopes
sjp.lmer(fit5c, type = "fe.slope", vars = c("xc", "style.f"))

# plot effects
sjp.lmer(fit5c, type = "eff")

# plot effects
sjp.lmer(fit5c, type = "pred", vars = "xc")
sjp.lmer(fit5c, type = "pred", vars = "style.f")

# plot effects
sjp.lmer(fit5c, type = "pred", vars = c("xc", "style.f"))

# plot effects
sjp.lmer(fit5c, type = "pred", 
         facet.grid = FALSE, 
         vars = c("xc", "style.f"))

# plot fixed effects depending on group levels
sjp.lmer(fit5c, type = "ri.slope")

# plot fixed effects correlation matrix
sjp.lmer(fit5c, type = "fe.cor")

# plot qq-plot of random effects
sjp.lmer(fit5c, type = "re.qq")

```



# Random intercepts & slopes {.tabset .tabset-fade .tabset-pills}  

Random intercepts & slopes

## Assignment 

### Teaching style

+ Try to see whether teaching style explains slope variance of schools.

+ Our dataset is too small!


## Solution

### Teaching style

```{r 2randa, message=FALSE, warning=FALSE,eval=FALSE}
# Add teaching style as a random slope
fit5d <- lmer(y ~ xc + style.f + (style.f |school.f/student.f) ,data = df)
summary(fit5d)
```

```{r 2randb, message=FALSE, warning=FALSE,eval=TRUE}
# This does work
fit5d <- lmer(y ~ xc  + (xc |school.f) ,data = df)
summary(fit5d)
```

